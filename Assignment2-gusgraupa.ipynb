{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                                            gusgraupa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: More Chinese-language experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will work with the Demo 2.1 - Chinese word segmentation on Chinese word segmentation.  The notebook trains a model that is very successful at determining word boundaries in Chinese text, where word boundaries are binary-encoded on a per-character basis with 1 being the first character of a word and 0 being the second character.  The model uses character embeddings and an LSTM to model the word boundaries.  You will copy and modify Demo 2.1's notebook as below and write up in the notebook Markdown what you did and how well it performed.\n",
    "\n",
    "This assignment is due at 23:59 on 2021 November 2.  It has 30 points and 15 bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data, padding (based on 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We add the <start> as \"ñ\" and <end> token as \"Ö\", and get the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chinese_data(inputfilename):\n",
    "    # the inputfile is in ud format\n",
    "    # returns a tuple of ([sentence], [collection_labels])\n",
    "    # collection_labels is 1 for the first characters of the word, and 0 for the others\n",
    "    \n",
    "    with open(inputfilename, \"r\") as inputfile:\n",
    "        sentences = []\n",
    "        collection_words = ['ñ']\n",
    "        collection_labels = []\n",
    "        for line in inputfile:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            #print(words)\n",
    "            if columns == []:\n",
    "                bla = ''.join(collection_words) + 'Ö'\n",
    "#                 sentences.append(bla)\n",
    "                sentences.append((bla, [1] + collection_labels))\n",
    "                collection_words = []\n",
    "                collection_labels = []\n",
    "                continue\n",
    "            collection_words.append(columns[1])\n",
    "            collection_labels += [1] + ([0] * (len(columns[1]) - 1)) # all but the first words will get a 0 label\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-train.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_chars(sentences):\n",
    "    # getting an index for each character in the set\n",
    "    megasentence = ''.join(sentences)\n",
    "    char_list = set()\n",
    "    for c in megasentence:\n",
    "        char_list.add(c) # getting all the characters\n",
    "    char_list = [0] + list(char_list) # we add a 0\n",
    "    return char_list, {char_list[x]:x for x in range(len(char_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did x[0] for x in train_sentences because the first function returns a tuple\n",
    "int_index, char_index = index_chars([x[0] for x in train_sentences + test_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(sentence, index):\n",
    "    # so everything is indexed as a number \n",
    "    return [index[x] for x in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_lengths(sentences, max_length, padding=0):\n",
    "    # we do padding to get it to the right shape so numpy doesna complain\n",
    "    return [x + ([padding] * (max_length - len(x))) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x, device=\"cpu\"):\n",
    "    converted = [(convert_sentence(x1[0], char_index), x1[1]) for x1 in x]\n",
    "    X, y = zip(*converted)\n",
    "    # we need to keep track of the actual lengths of the sentences\n",
    "    lengths = [len(x2) for x2 in X]\n",
    "    padded_X = pad_lengths(X, max(lengths)) # padding the sentences\n",
    "    Xt = torch.LongTensor(padded_X).to(device)\n",
    "    padded_y = pad_lengths(y, max(lengths), padding=-1) # padding the labels WITH -1!!\n",
    "    yt = torch.LongTensor(padded_y).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    return Xt, lengths_t, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu tensor containing input sent, gpu tensor based on lenghts of each sentence, and the -1 padded class labels\n",
    "train_X_tensor, train_lengths_tensor, train_y_tensor = create_dataset(train_sentences, device)\n",
    "test_X_tensor, test_lengths_tensor, test_y_tensor = create_dataset(test_sentences, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing the sequences for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching (based on 1.0, 1.1, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, X, lengths, y, device, batch_size=50, max_iter=None):\n",
    "        self.X = X\n",
    "        self.lengths = lengths # We need the lengths to efficiently use the padding.\n",
    "        self.y = y\n",
    "        self.device = device\n",
    "        self.batch_size=batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.curr_iter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.curr_iter == self.max_iter:\n",
    "            raise StopIteration\n",
    "        permutation = torch.randperm(self.X.size()[0], device=self.device)\n",
    "        permX = self.X[permutation]\n",
    "        permlengths = self.lengths[permutation]\n",
    "        permy = self.y[permutation]\n",
    "        splitX = torch.split(permX, self.batch_size)\n",
    "        splitlengths = torch.split(permlengths, self.batch_size)\n",
    "        splity = torch.split(permy, self.batch_size)\n",
    "        \n",
    "        self.curr_iter += 1\n",
    "        return zip(splitX, splitlengths, splity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Batcher(train_X_tensor, train_lengths_tensor, train_y_tensor, device, max_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 - Character segmentation\n",
    "From Asad's notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    All models are trained with 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter0(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # what's the point of this?\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, 150, batch_first=True)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.lin = nn.Linear(150, 2)\n",
    "        self.softmax = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        embs = self.emb(x)\n",
    "        packed = pack_padded_sequence(embs, lengths.to(\"cpu\"), batch_first=True, enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "        output2 = self.sig1(unpacked)\n",
    "        output3 = self.lin(output2)\n",
    "        return self.softmax(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train0(X, lengths, y, vocab_size, emb_size, batch_size, epochs, device, model=None):\n",
    "    b = Batcher(X, lengths, y, device, batch_size=batch_size, max_iter=epochs)\n",
    "    \n",
    "#     if not model:\n",
    "#         m = Segmenter(vocab_size, emb_size).to(device)\n",
    "#     else:\n",
    "#         m = model\n",
    "\n",
    "    m = Segmenter0(vocab_size, emb_size).to(device)\n",
    "    loss = nn.NLLLoss(ignore_index=-1)\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "    epoch = 0\n",
    "    \n",
    "    loss_plot = []\n",
    "    \n",
    "    for split in b:\n",
    "        tot_loss = 0\n",
    "        for batch in split:\n",
    "            optimizer.zero_grad()\n",
    "            o = m(batch[0], batch[1])\n",
    "            l = loss(o.permute(0,2,1), batch[2][:, :max(batch[1])])\n",
    "            \n",
    "            loss_plot.append(l)\n",
    "            \n",
    "            tot_loss += l\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Total loss in epoch {} is {}.\".format(epoch, tot_loss))\n",
    "        epoch += 1\n",
    "    \n",
    "    plt.plot(loss_plot)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 0 is 33.425968170166016.\n",
      "Total loss in epoch 1 is 17.24395179748535.\n",
      "Total loss in epoch 2 is 11.733686447143555.\n",
      "Total loss in epoch 3 is 8.110830307006836.\n",
      "Total loss in epoch 4 is 5.73553991317749.\n",
      "Total loss in epoch 5 is 4.519376277923584.\n",
      "Total loss in epoch 6 is 3.4346179962158203.\n",
      "Total loss in epoch 7 is 2.6329524517059326.\n",
      "Total loss in epoch 8 is 2.2823922634124756.\n",
      "Total loss in epoch 9 is 1.9920775890350342.\n",
      "Total loss in epoch 10 is 1.801770567893982.\n",
      "Total loss in epoch 11 is 1.6543806791305542.\n",
      "Total loss in epoch 12 is 1.1278927326202393.\n",
      "Total loss in epoch 13 is 0.8453371524810791.\n",
      "Total loss in epoch 14 is 0.6275399923324585.\n",
      "Total loss in epoch 15 is 0.5688338279724121.\n",
      "Total loss in epoch 16 is 0.6711459159851074.\n",
      "Total loss in epoch 17 is 0.9374004006385803.\n",
      "Total loss in epoch 18 is 1.831060528755188.\n",
      "Total loss in epoch 19 is 2.030604124069214.\n",
      "Total loss in epoch 20 is 1.4820802211761475.\n",
      "Total loss in epoch 21 is 0.7925536036491394.\n",
      "Total loss in epoch 22 is 0.4352429211139679.\n",
      "Total loss in epoch 23 is 0.23858897387981415.\n",
      "Total loss in epoch 24 is 0.13397309184074402.\n",
      "Total loss in epoch 25 is 0.08355804532766342.\n",
      "Total loss in epoch 26 is 0.05908537656068802.\n",
      "Total loss in epoch 27 is 0.047804322093725204.\n",
      "Total loss in epoch 28 is 0.040931809693574905.\n",
      "Total loss in epoch 29 is 0.03507089242339134.\n",
      "Total loss in epoch 30 is 0.030928663909435272.\n",
      "Total loss in epoch 31 is 0.027842972427606583.\n",
      "Total loss in epoch 32 is 0.025149879977107048.\n",
      "Total loss in epoch 33 is 0.022862449288368225.\n",
      "Total loss in epoch 34 is 0.021614760160446167.\n",
      "Total loss in epoch 35 is 0.019015848636627197.\n",
      "Total loss in epoch 36 is 0.017674708738923073.\n",
      "Total loss in epoch 37 is 0.016190001741051674.\n",
      "Total loss in epoch 38 is 0.015095395967364311.\n",
      "Total loss in epoch 39 is 0.013955785892903805.\n",
      "Total loss in epoch 40 is 0.013177038170397282.\n",
      "Total loss in epoch 41 is 0.01230625994503498.\n",
      "Total loss in epoch 42 is 0.011566011235117912.\n",
      "Total loss in epoch 43 is 0.010934066027402878.\n",
      "Total loss in epoch 44 is 0.010451718233525753.\n",
      "Total loss in epoch 45 is 0.00966185424476862.\n",
      "Total loss in epoch 46 is 0.009607968851923943.\n",
      "Total loss in epoch 47 is 0.008936960250139236.\n",
      "Total loss in epoch 48 is 0.008497648872435093.\n",
      "Total loss in epoch 49 is 0.007990292273461819.\n",
      "Total loss in epoch 50 is 0.008010029792785645.\n",
      "Total loss in epoch 51 is 0.0074370973743498325.\n",
      "Total loss in epoch 52 is 0.007057747803628445.\n",
      "Total loss in epoch 53 is 0.00675890501588583.\n",
      "Total loss in epoch 54 is 0.0067123426124453545.\n",
      "Total loss in epoch 55 is 0.006298108492046595.\n",
      "Total loss in epoch 56 is 0.0062232231721282005.\n",
      "Total loss in epoch 57 is 0.0060216947458684444.\n",
      "Total loss in epoch 58 is 0.005867406725883484.\n",
      "Total loss in epoch 59 is 0.005509942304342985.\n",
      "Total loss in epoch 60 is 0.005474867299199104.\n",
      "Total loss in epoch 61 is 0.005031357519328594.\n",
      "Total loss in epoch 62 is 0.004995916038751602.\n",
      "Total loss in epoch 63 is 0.004768237937241793.\n",
      "Total loss in epoch 64 is 0.004802551586180925.\n",
      "Total loss in epoch 65 is 0.004456812981516123.\n",
      "Total loss in epoch 66 is 0.004511542152613401.\n",
      "Total loss in epoch 67 is 0.004530006553977728.\n",
      "Total loss in epoch 68 is 0.004211680497974157.\n",
      "Total loss in epoch 69 is 0.004131417255848646.\n",
      "Total loss in epoch 70 is 0.00402514822781086.\n",
      "Total loss in epoch 71 is 0.0040790848433971405.\n",
      "Total loss in epoch 72 is 0.0038721910677850246.\n",
      "Total loss in epoch 73 is 0.0039244163781404495.\n",
      "Total loss in epoch 74 is 0.003770019393414259.\n",
      "Total loss in epoch 75 is 0.003607915248721838.\n",
      "Total loss in epoch 76 is 0.0036669455002993345.\n",
      "Total loss in epoch 77 is 0.003567712614312768.\n",
      "Total loss in epoch 78 is 0.003722312394529581.\n",
      "Total loss in epoch 79 is 0.003464275738224387.\n",
      "Total loss in epoch 80 is 0.003982649650424719.\n",
      "Total loss in epoch 81 is 0.0034717742819339037.\n",
      "Total loss in epoch 82 is 0.0034175473265349865.\n",
      "Total loss in epoch 83 is 0.0033121139276772738.\n",
      "Total loss in epoch 84 is 0.0032548767048865557.\n",
      "Total loss in epoch 85 is 0.0032945824787020683.\n",
      "Total loss in epoch 86 is 0.0032196366228163242.\n",
      "Total loss in epoch 87 is 0.003401915542781353.\n",
      "Total loss in epoch 88 is 0.003496830817312002.\n",
      "Total loss in epoch 89 is 0.003060061251744628.\n",
      "Total loss in epoch 90 is 0.003682667389512062.\n",
      "Total loss in epoch 91 is 6.954837799072266.\n",
      "Total loss in epoch 92 is 9.187979698181152.\n",
      "Total loss in epoch 93 is 4.592923164367676.\n",
      "Total loss in epoch 94 is 2.5609331130981445.\n",
      "Total loss in epoch 95 is 1.4270330667495728.\n",
      "Total loss in epoch 96 is 0.821814775466919.\n",
      "Total loss in epoch 97 is 0.49022310972213745.\n",
      "Total loss in epoch 98 is 0.3180084228515625.\n",
      "Total loss in epoch 99 is 0.21441411972045898.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model0 = train0(train_X_tensor, train_lengths_tensor, train_y_tensor, len(int_index), 200, 50, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segmenter0(\n",
       "  (emb): Embedding(3650, 200, padding_idx=0)\n",
       "  (lstm): LSTM(200, 150, batch_first=True)\n",
       "  (sig1): Sigmoid()\n",
       "  (lin): Linear(in_features=150, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Sentence generation (15 points).\n",
    "\n",
    "Convert the model in Demo 2.1 into a character-based sentence generator. (Strip out the word segmentation objective.)  The model should, given a start symbol, produce a variety of sentences that terminate with a stop symbol (you will have to add these to the data).  The sentences that it generates should be of reasonable average length compared to the sentences in the training corpus (this needn't be precise). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    I changed the size of the output of the linear layer to be the size of the vocabulary, to get the probability for each character of the vocabulary.\n",
    "    I removed the sigmoid function because I was getting many zeros as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter1(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, 150, batch_first=True)\n",
    "#         self.sig1 = nn.Sigmoid()\n",
    "        self.lin = nn.Linear(150, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        embs = self.emb(x)\n",
    "        packed = pack_padded_sequence(embs, lengths.to(\"cpu\"), batch_first=True, enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "#         output2 = self.sig1(unpacked)\n",
    "#         output3 = self.lin(output2)\n",
    "        output3 = self.lin(unpacked)\n",
    "        output4 = self.softmax(output3)\n",
    "        \n",
    "        return output4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In the loss, we :\n",
    "    - get rid of the last element of the second dimension of the output of the model, which would be the end token\n",
    "    - remove the first element of the second dimension of the target of the model, because it is the start token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(X, lengths, y, vocab_size, emb_size, batch_size, epochs, device, model=None):\n",
    "    b = Batcher(X, lengths, y, device, batch_size=batch_size, max_iter=epochs)\n",
    "    \n",
    "#     if not model:\n",
    "#         m = Segmenter(vocab_size, emb_size).to(device)\n",
    "#     else:\n",
    "#         m = model\n",
    "    m = Segmenter1(vocab_size, emb_size).to(device)\n",
    "    loss = nn.NLLLoss(ignore_index=-1)\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "    epoch = 0\n",
    "    \n",
    "    lengths = []\n",
    "    loss_plot = []\n",
    "    \n",
    "    for split in b:\n",
    "        tot_loss = 0\n",
    "        for batch in split:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lengths.append(batch[1])\n",
    "            \n",
    "            o = m(batch[0], batch[1]) # batch[1] is the length of the sentences\n",
    "\n",
    "            l = loss(o[:, :-1, :].permute(0,2,1), batch[0][:, 1:max(batch[1])])\n",
    "            \n",
    "            loss_plot.append(l)\n",
    "            \n",
    "            tot_loss += l\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Total loss in epoch {} is {}.\".format(epoch, tot_loss))\n",
    "        epoch += 1\n",
    "        \n",
    "    plt.plot(loss_plot)\n",
    "    \n",
    "#     perplexity  = torch.exp(tot_loss)\n",
    "    \n",
    "#     print('perplexity:', perplexity.item())\n",
    "    \n",
    "    return m, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 0 is 571.1435546875.\n",
      "Total loss in epoch 1 is 506.4555358886719.\n",
      "Total loss in epoch 2 is 459.3341369628906.\n",
      "Total loss in epoch 3 is 415.0297546386719.\n",
      "Total loss in epoch 4 is 372.6875915527344.\n",
      "Total loss in epoch 5 is 331.84820556640625.\n",
      "Total loss in epoch 6 is 292.5474853515625.\n",
      "Total loss in epoch 7 is 254.5089874267578.\n",
      "Total loss in epoch 8 is 219.49356079101562.\n",
      "Total loss in epoch 9 is 187.9837188720703.\n",
      "Total loss in epoch 10 is 161.32431030273438.\n",
      "Total loss in epoch 11 is 141.4477996826172.\n",
      "Total loss in epoch 12 is 126.2373275756836.\n",
      "Total loss in epoch 13 is 112.74423217773438.\n",
      "Total loss in epoch 14 is 103.81791687011719.\n",
      "Total loss in epoch 15 is 98.4115982055664.\n",
      "Total loss in epoch 16 is 93.44049835205078.\n",
      "Total loss in epoch 17 is 87.68924713134766.\n",
      "Total loss in epoch 18 is 82.98928833007812.\n",
      "Total loss in epoch 19 is 83.2125473022461.\n",
      "Total loss in epoch 20 is 78.27418518066406.\n",
      "Total loss in epoch 21 is 75.6357421875.\n",
      "Total loss in epoch 22 is 73.21231079101562.\n",
      "Total loss in epoch 23 is 70.89715576171875.\n",
      "Total loss in epoch 24 is 67.58634185791016.\n",
      "Total loss in epoch 25 is 66.42166900634766.\n",
      "Total loss in epoch 26 is 65.94599914550781.\n",
      "Total loss in epoch 27 is 64.1961441040039.\n",
      "Total loss in epoch 28 is 63.79779815673828.\n",
      "Total loss in epoch 29 is 62.001739501953125.\n",
      "Total loss in epoch 30 is 61.14961242675781.\n",
      "Total loss in epoch 31 is 59.329307556152344.\n",
      "Total loss in epoch 32 is 57.3734016418457.\n",
      "Total loss in epoch 33 is 57.880516052246094.\n",
      "Total loss in epoch 34 is 56.3587646484375.\n",
      "Total loss in epoch 35 is 55.64006042480469.\n",
      "Total loss in epoch 36 is 55.474212646484375.\n",
      "Total loss in epoch 37 is 53.9145393371582.\n",
      "Total loss in epoch 38 is 53.31947708129883.\n",
      "Total loss in epoch 39 is 53.15361785888672.\n",
      "Total loss in epoch 40 is 51.67584991455078.\n",
      "Total loss in epoch 41 is 51.0052375793457.\n",
      "Total loss in epoch 42 is 51.00484085083008.\n",
      "Total loss in epoch 43 is 49.91762924194336.\n",
      "Total loss in epoch 44 is 50.104793548583984.\n",
      "Total loss in epoch 45 is 48.79303741455078.\n",
      "Total loss in epoch 46 is 48.456993103027344.\n",
      "Total loss in epoch 47 is 48.78651809692383.\n",
      "Total loss in epoch 48 is 48.43818664550781.\n",
      "Total loss in epoch 49 is 46.115970611572266.\n",
      "Total loss in epoch 50 is 45.27417755126953.\n",
      "Total loss in epoch 51 is 45.002166748046875.\n",
      "Total loss in epoch 52 is 44.99998092651367.\n",
      "Total loss in epoch 53 is 45.71105194091797.\n",
      "Total loss in epoch 54 is 44.91802978515625.\n",
      "Total loss in epoch 55 is 44.565799713134766.\n",
      "Total loss in epoch 56 is 45.66651916503906.\n",
      "Total loss in epoch 57 is 45.38825988769531.\n",
      "Total loss in epoch 58 is 44.08116149902344.\n",
      "Total loss in epoch 59 is 45.795257568359375.\n",
      "Total loss in epoch 60 is 46.645912170410156.\n",
      "Total loss in epoch 61 is 44.63910675048828.\n",
      "Total loss in epoch 62 is 43.45872116088867.\n",
      "Total loss in epoch 63 is 42.237144470214844.\n",
      "Total loss in epoch 64 is 43.1248664855957.\n",
      "Total loss in epoch 65 is 42.24810791015625.\n",
      "Total loss in epoch 66 is 41.87861251831055.\n",
      "Total loss in epoch 67 is 41.39828109741211.\n",
      "Total loss in epoch 68 is 42.66172409057617.\n",
      "Total loss in epoch 69 is 42.25962448120117.\n",
      "Total loss in epoch 70 is 41.884010314941406.\n",
      "Total loss in epoch 71 is 41.408607482910156.\n",
      "Total loss in epoch 72 is 41.28584289550781.\n",
      "Total loss in epoch 73 is 40.80265426635742.\n",
      "Total loss in epoch 74 is 40.54703140258789.\n",
      "Total loss in epoch 75 is 40.723384857177734.\n",
      "Total loss in epoch 76 is 40.70962142944336.\n",
      "Total loss in epoch 77 is 40.47008514404297.\n",
      "Total loss in epoch 78 is 39.89689636230469.\n",
      "Total loss in epoch 79 is 39.86239242553711.\n",
      "Total loss in epoch 80 is 39.04925537109375.\n",
      "Total loss in epoch 81 is 39.620304107666016.\n",
      "Total loss in epoch 82 is 39.63962173461914.\n",
      "Total loss in epoch 83 is 39.15391159057617.\n",
      "Total loss in epoch 84 is 39.652976989746094.\n",
      "Total loss in epoch 85 is 40.11620330810547.\n",
      "Total loss in epoch 86 is 38.88053894042969.\n",
      "Total loss in epoch 87 is 39.32788848876953.\n",
      "Total loss in epoch 88 is 38.746089935302734.\n",
      "Total loss in epoch 89 is 37.912322998046875.\n",
      "Total loss in epoch 90 is 37.98470687866211.\n",
      "Total loss in epoch 91 is 37.28306198120117.\n",
      "Total loss in epoch 92 is 37.49793243408203.\n",
      "Total loss in epoch 93 is 38.212467193603516.\n",
      "Total loss in epoch 94 is 38.58547592163086.\n",
      "Total loss in epoch 95 is 38.94655227661133.\n",
      "Total loss in epoch 96 is 39.998435974121094.\n",
      "Total loss in epoch 97 is 39.947776794433594.\n",
      "Total loss in epoch 98 is 40.428096771240234.\n",
      "Total loss in epoch 99 is 38.9677848815918.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 200 is emb_size\n",
    "# 50 is batch_size\n",
    "# 30 is number of epochs\n",
    "model1, lengths1 = train1(train_X_tensor, train_lengths_tensor, train_y_tensor, len(int_index), 200, 50, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segmenter1(\n",
       "  (emb): Embedding(3650, 200, padding_idx=0)\n",
       "  (lstm): LSTM(200, 150, batch_first=True)\n",
       "  (lin): Linear(in_features=150, out_features=3650, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The function \"generating\" creates a sentence given the start symbol (ñ) and a random seed (from possible first characters in the train sentences). The length of the sentence is a random choice from the train_sentences length, or shorter if the model produces a stop symbol (Ö)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting list of possible lengths of sentences\n",
    "l = []\n",
    "for x in lengths1:\n",
    "    l.extend(x.tolist())\n",
    "\n",
    "sentences = train_sentences.extend(test_sentences)\n",
    "# getting the possible seeds of the sentences\n",
    "seeds = []\n",
    "for a, _ in train_sentences:\n",
    "    seeds.extend(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generating(lengths, seeds, char_index, model):\n",
    "    # the index of the start symbol \"ñ\" is 2925\n",
    "    # the index of the end symbol \"Ö\" is 149\n",
    "    \n",
    "    # getting random sentence length\n",
    "    sent_len = torch.tensor(random.choice(lengths)).to(device).unsqueeze(0)\n",
    "    sent_len = sent_len.int().to(device)\n",
    "    \n",
    "    # getting index of random seed\n",
    "    seed = random.choice(seeds)\n",
    "    seed_idx = char_index[seed]\n",
    "\n",
    "    # generating the original tensor\n",
    "    original = torch.zeros(sent_len)\n",
    "    original[0] = char_index['ñ']\n",
    "    original[1] = seed_idx\n",
    "    original = original.long().unsqueeze(0).to(device)\n",
    "    \n",
    "    # passing it through the model\n",
    "    for e in range(int(sent_len)-3):\n",
    "        out = model(original, sent_len)\n",
    "        m = torch.argmax(out, dim=2)\n",
    "#         print(m)\n",
    "        original[0][e+2] = m[0][e+2]\n",
    "        \n",
    "        if m[0][e+2] == char_index['Ö']:\n",
    "            break\n",
    "    \n",
    "    original = original.squeeze(0)\n",
    "    if char_index['Ö'] not in original:\n",
    "        original[int(sent_len)-1] = char_index['Ö']\n",
    "    \n",
    "    inlist = original.tolist()\n",
    "    text = ''\n",
    "    for num in inlist:\n",
    "        char = str(list(char_index.keys())[list(char_index.values()).index(num)])\n",
    "        text += char\n",
    "        if char == 'Ö':\n",
    "            break\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ñ據出的力，向進準向軍隊得自出，送會射表將出，手與想，助嚴了毀，任舊政、埔台）。，管確，洛（i直Ö'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generating(l, seeds, char_index, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ñ國法軍逃，任軍改為。，稱為籤給器的量。。，出Ö'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generating(l, seeds, char_index, model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Dual objectives (10 points)\n",
    "Copy the notebook from part 1 and augment the copy by adding back the word segmentation objective, as a second objective with its own loss.  (You could also in theory do Part 1 and Part 2 in reverse, by adding sentence generation with dual objectives first and then stripping out the word segmentation objective; this is equivalent.)  \n",
    "Note that multiple losses can be combined by simple, possibly weighted addition -- backpropagation works entirely correctly on the combined loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From the original model, I added a second linear layer with an output size as the size of the vocabulary. It returns two tensors, one for the segmentation objective and one for the prediction objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, 150, batch_first=True)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.lin1 = nn.Linear(150, 2)\n",
    "        self.lin2 = nn.Linear(150, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        embs = self.emb(x)\n",
    "        packed = pack_padded_sequence(embs, lengths.to(\"cpu\"), batch_first=True, enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "        output2 = self.sig1(unpacked)\n",
    "        output3_segm = self.lin1(output2)\n",
    "        output3_pred = self.lin2(unpacked) # no sigmoid for text prediction\n",
    "        \n",
    "        return (self.softmax(output3_segm), self.softmax(output3_pred))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We calculate two losses, one for each objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, lengths, y, vocab_size, emb_size, batch_size, epochs, device, model=None):\n",
    "    b = Batcher(X, lengths, y, device, batch_size=batch_size, max_iter=epochs)\n",
    "    \n",
    "#     if not model:\n",
    "#         m = Segmenter(vocab_size, emb_size).to(device)\n",
    "#     else:\n",
    "#         m = model\n",
    "    \n",
    "    m = Segmenter(vocab_size, emb_size).to(device)\n",
    "    loss = nn.NLLLoss(ignore_index=-1)\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "    epoch = 0\n",
    "    \n",
    "    loss_plot1 = []\n",
    "    loss_plot2 = []\n",
    "    loss_summ = []\n",
    "    \n",
    "    lengths = []\n",
    "    \n",
    "    for split in b:\n",
    "        tot_loss = 0\n",
    "        for batch in split:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lengths.append(batch[1])\n",
    "            \n",
    "            o_segm, o_pred = m(batch[0], batch[1]) # batch[1] is the length of the sentences\n",
    "\n",
    "            l1 = loss(o_segm.permute(0,2,1), batch[2][:, :max(batch[1])])\n",
    "            l2 = loss(o_pred[:, :-1, :].permute(0,2,1), batch[0][:, 1:max(batch[1])])\n",
    "            \n",
    "            loss_plot1.append(l1)\n",
    "            loss_plot2.append(l2)\n",
    "            \n",
    "            superl = l1 + l2\n",
    "            \n",
    "            loss_summ.append(superl)\n",
    "            \n",
    "            tot_loss += superl\n",
    "            \n",
    "            superl.backward()\n",
    "            \n",
    "#             l1.backward(retain_graph=True)\n",
    "#             l2.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        print(\"Total loss in epoch {} is {}.\".format(epoch, tot_loss))\n",
    "        epoch += 1\n",
    "    \n",
    "    plt.plot(loss_plot1, label = 'Segmentation loss')\n",
    "    plt.plot(loss_plot2, label = 'Prediction loss')\n",
    "    plt.plot(loss_summ, label = 'Total loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    \n",
    "#     perplexity  = torch.exp(tot_loss)\n",
    "    \n",
    "#     print('perplexity:', perplexity.item())\n",
    "    \n",
    "    return m, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 0 is 613.0823364257812.\n",
      "Total loss in epoch 1 is 531.5040893554688.\n",
      "Total loss in epoch 2 is 476.08526611328125.\n",
      "Total loss in epoch 3 is 429.22210693359375.\n",
      "Total loss in epoch 4 is 385.42529296875.\n",
      "Total loss in epoch 5 is 344.59014892578125.\n",
      "Total loss in epoch 6 is 304.8947448730469.\n",
      "Total loss in epoch 7 is 267.8619384765625.\n",
      "Total loss in epoch 8 is 232.54722595214844.\n",
      "Total loss in epoch 9 is 200.01365661621094.\n",
      "Total loss in epoch 10 is 176.8213653564453.\n",
      "Total loss in epoch 11 is 156.35662841796875.\n",
      "Total loss in epoch 12 is 139.55943298339844.\n",
      "Total loss in epoch 13 is 129.03533935546875.\n",
      "Total loss in epoch 14 is 121.10475158691406.\n",
      "Total loss in epoch 15 is 114.87969207763672.\n",
      "Total loss in epoch 16 is 108.7334976196289.\n",
      "Total loss in epoch 17 is 104.73020935058594.\n",
      "Total loss in epoch 18 is 101.44131469726562.\n",
      "Total loss in epoch 19 is 97.85631561279297.\n",
      "Total loss in epoch 20 is 94.05367279052734.\n",
      "Total loss in epoch 21 is 91.00130462646484.\n",
      "Total loss in epoch 22 is 88.23133850097656.\n",
      "Total loss in epoch 23 is 85.32477569580078.\n",
      "Total loss in epoch 24 is 84.10183715820312.\n",
      "Total loss in epoch 25 is 82.0091552734375.\n",
      "Total loss in epoch 26 is 81.4354476928711.\n",
      "Total loss in epoch 27 is 78.0859375.\n",
      "Total loss in epoch 28 is 76.28790283203125.\n",
      "Total loss in epoch 29 is 75.60395050048828.\n",
      "Total loss in epoch 30 is 75.2310791015625.\n",
      "Total loss in epoch 31 is 74.68688201904297.\n",
      "Total loss in epoch 32 is 71.93240356445312.\n",
      "Total loss in epoch 33 is 72.0963134765625.\n",
      "Total loss in epoch 34 is 70.8971176147461.\n",
      "Total loss in epoch 35 is 69.2406997680664.\n",
      "Total loss in epoch 36 is 67.58903503417969.\n",
      "Total loss in epoch 37 is 67.426513671875.\n",
      "Total loss in epoch 38 is 66.460205078125.\n",
      "Total loss in epoch 39 is 64.95415496826172.\n",
      "Total loss in epoch 40 is 64.83556365966797.\n",
      "Total loss in epoch 41 is 65.0916976928711.\n",
      "Total loss in epoch 42 is 64.54171752929688.\n",
      "Total loss in epoch 43 is 64.24127197265625.\n",
      "Total loss in epoch 44 is 62.71575164794922.\n",
      "Total loss in epoch 45 is 61.21160888671875.\n",
      "Total loss in epoch 46 is 59.2771110534668.\n",
      "Total loss in epoch 47 is 60.18897247314453.\n",
      "Total loss in epoch 48 is 58.14693069458008.\n",
      "Total loss in epoch 49 is 59.59351348876953.\n",
      "Total loss in epoch 50 is 58.64474868774414.\n",
      "Total loss in epoch 51 is 58.40205001831055.\n",
      "Total loss in epoch 52 is 58.78213882446289.\n",
      "Total loss in epoch 53 is 57.896080017089844.\n",
      "Total loss in epoch 54 is 59.984676361083984.\n",
      "Total loss in epoch 55 is 57.99263000488281.\n",
      "Total loss in epoch 56 is 56.90119171142578.\n",
      "Total loss in epoch 57 is 56.2926139831543.\n",
      "Total loss in epoch 58 is 54.85268020629883.\n",
      "Total loss in epoch 59 is 53.562767028808594.\n",
      "Total loss in epoch 60 is 51.81800079345703.\n",
      "Total loss in epoch 61 is 51.723777770996094.\n",
      "Total loss in epoch 62 is 51.5667610168457.\n",
      "Total loss in epoch 63 is 52.30976486206055.\n",
      "Total loss in epoch 64 is 54.61298370361328.\n",
      "Total loss in epoch 65 is 54.866390228271484.\n",
      "Total loss in epoch 66 is 54.15001678466797.\n",
      "Total loss in epoch 67 is 52.46367263793945.\n",
      "Total loss in epoch 68 is 50.928524017333984.\n",
      "Total loss in epoch 69 is 50.839500427246094.\n",
      "Total loss in epoch 70 is 51.63508987426758.\n",
      "Total loss in epoch 71 is 49.362483978271484.\n",
      "Total loss in epoch 72 is 51.46991729736328.\n",
      "Total loss in epoch 73 is 51.47908020019531.\n",
      "Total loss in epoch 76 is 50.897552490234375.\n",
      "Total loss in epoch 77 is 48.176273345947266.\n",
      "Total loss in epoch 78 is 48.81657409667969.\n",
      "Total loss in epoch 79 is 49.14298629760742.\n",
      "Total loss in epoch 80 is 48.60614776611328.\n",
      "Total loss in epoch 81 is 47.7420539855957.\n",
      "Total loss in epoch 82 is 47.65761184692383.\n",
      "Total loss in epoch 83 is 48.31364822387695.\n",
      "Total loss in epoch 84 is 47.57154083251953.\n",
      "Total loss in epoch 85 is 52.628639221191406.\n",
      "Total loss in epoch 86 is 56.69640350341797.\n",
      "Total loss in epoch 87 is 54.555397033691406.\n",
      "Total loss in epoch 88 is 53.360984802246094.\n",
      "Total loss in epoch 89 is 52.80803680419922.\n",
      "Total loss in epoch 90 is 51.37494659423828.\n",
      "Total loss in epoch 91 is 49.04031753540039.\n",
      "Total loss in epoch 92 is 47.29458999633789.\n",
      "Total loss in epoch 93 is 45.06400680541992.\n",
      "Total loss in epoch 94 is 45.609066009521484.\n",
      "Total loss in epoch 95 is 45.285823822021484.\n",
      "Total loss in epoch 96 is 45.4939079284668.\n",
      "Total loss in epoch 97 is 43.531036376953125.\n",
      "Total loss in epoch 98 is 45.44517135620117.\n",
      "Total loss in epoch 99 is 45.349910736083984.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 200 is emb_size\n",
    "# 50 is batch_size\n",
    "# 30 is number of epochs\n",
    "model, lengths = train(train_X_tensor, train_lengths_tensor, train_y_tensor, len(int_index), 200, 50, 100, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The total loss and the prediction loss are almost the same, because the segmentation loss is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segmenter(\n",
       "  (emb): Embedding(3650, 200, padding_idx=0)\n",
       "  (lstm): LSTM(200, 150, batch_first=True)\n",
       "  (sig1): Sigmoid()\n",
       "  (lin1): Linear(in_features=150, out_features=2, bias=True)\n",
       "  (lin2): Linear(in_features=150, out_features=3650, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Analysis (5 points)\n",
    "You now have three models.  The original word segmentation model, a sentence generation model, and a dual sentence-generation/word segmentation model. \n",
    "\n",
    "Compare the performance on the test data of the original word segmentation model between the original objective and the dual objective model.  \n",
    "- In how many iterations do the models converge?  \n",
    "- What are their final F1 and accuracy scores once they've converged? \n",
    "- Are they any different?  If so, why?\n",
    "\n",
    "Make the same comparison between the sentence generation model and the dual-objective model, except the performance measure is the per-word perplexity on the text (_test?_) corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - word segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From Asad's word segmentation model (model0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segmenter(\n",
       "  (emb): Embedding(3650, 200, padding_idx=0)\n",
       "  (lstm): LSTM(200, 150, batch_first=True)\n",
       "  (sig1): Sigmoid()\n",
       "  (lin1): Linear(in_features=150, out_features=2, bias=True)\n",
       "  (lin2): Linear(in_features=150, out_features=3650, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0.eval()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rawpredictions0 = model0(test_X_tensor, test_lengths_tensor)\n",
    "predictions0 = torch.argmax(rawpredictions0, 2)\n",
    "\n",
    "collectpreds0 = []\n",
    "collecty0 = []\n",
    "\n",
    "for i in range(test_X_tensor.size(0)):\n",
    "    collectpreds0.append(predictions0[i][:test_lengths_tensor[i]])\n",
    "    collecty0.append(test_y_tensor[i][:test_lengths_tensor[i]])\n",
    "    \n",
    "allpreds0 = torch.cat(collectpreds0).float()\n",
    "classes0 = torch.cat(collecty0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp0 = sum(classes0 * allpreds0)\n",
    "fp0 = sum(classes0 * (~allpreds0.bool()).float())\n",
    "tn0 = sum((~classes0.bool()).float() * (~allpreds0.bool()).float())\n",
    "fn0 = sum((~classes0.bool()).float() * allpreds0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9393, device='cuda:3')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy0 = (tp0 + tn0) / (tp0 + fp0 + tn0 + fn0)\n",
    "accuracy0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9506, device='cuda:3')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall0 = tp0 / (tp0 + fn0)\n",
    "recall0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9539, device='cuda:3')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision0 = tp0 / (tp0 + fp0)\n",
    "precision0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9522, device='cuda:3')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_0 = (2 * recall0 * precision0) / (recall0 + precision0)\n",
    "f1_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From the dual model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rawpredictions, bla = model(test_X_tensor, test_lengths_tensor)\n",
    "predictions = torch.argmax(rawpredictions, 2)\n",
    "collectpreds = []\n",
    "collecty = []\n",
    "for i in range(test_X_tensor.size(0)):\n",
    "    collectpreds.append(predictions[i][:test_lengths_tensor[i]])\n",
    "    collecty.append(test_y_tensor[i][:test_lengths_tensor[i]])\n",
    "allpreds = torch.cat(collectpreds).float()\n",
    "classes = torch.cat(collecty).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = sum(classes * allpreds)\n",
    "fp = sum(classes * (~allpreds.bool()).float())\n",
    "tn = sum((~classes.bool()).float() * (~allpreds.bool()).float())\n",
    "fn = sum((~classes.bool()).float() * allpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9320, device='cuda:3')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9419, device='cuda:3')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = tp / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9517, device='cuda:3')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = tp / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9468, device='cuda:3')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = (2 * recall * precision) / (recall + precision)\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Asad's word segmentation model seems to converge at epoch number 40, while the model for part 1 converges at epoch number 80, since the loss does not seem to get that much lower. The dual model converges around epoch 80 as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Accuracy | F1score |\n",
    "| --- | --- | --- |\n",
    "| Asad's | 0.9393 | 0.9522|\n",
    "| Dual | 0.9320 | 0.9468|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The f1 score for Asad's word segmentation model is 95,22% and the f1 score for the dual model is 94,68% when checking the segmentation objective. The accuracy for the first model is 93.93% and 93.2% in the second one.\n",
    "    The dual model's f1 is 0,54% lower, and the accuracy, 0.73% lower. They are quite similar, probably because both models have the same layers and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation - text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_time_simple(X, lengths, y, vocab_size, emb_size, batch_size, epochs, device, model):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    b = Batcher(X, lengths, y, device, batch_size=batch_size, max_iter=epochs)\n",
    "    for split in b:\n",
    "            tot_loss = 0\n",
    "            for batch in split:\n",
    "                out = model(batch[0], batch[1])\n",
    "                l = loss(out[:, :-1, :].permute(0,2,1), batch[0][:, 1:max(batch[1])])\n",
    "                tot_loss += l\n",
    "    \n",
    "    perplexity  = torch.exp(tot_loss/50)\n",
    "    \n",
    "    print('Total perplexity:', perplexity.item())\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total perplexity: 1.64597749710083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.64597749710083"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_time_simple(test_X_tensor, test_lengths_tensor, test_y_tensor, len(int_index), 200, 50, 30, device, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_time_dual(X, lengths, y, vocab_size, emb_size, batch_size, epochs, device, model):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    b = Batcher(X, lengths, y, device, batch_size=batch_size, max_iter=epochs)\n",
    "    for split in b:\n",
    "            tot_loss = 0\n",
    "            for batch in split:\n",
    "                _, out = model(batch[0], batch[1])\n",
    "                l = loss(out[:, :-1, :].permute(0,2,1), batch[0][:, 1:max(batch[1])])\n",
    "                tot_loss += l\n",
    "    \n",
    "    perplexity  = torch.exp(tot_loss/50)\n",
    "    \n",
    "    print('Total perplexity:', perplexity.item())\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total perplexity: 1.6071158647537231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6071158647537231"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X_tensor, train_lengths_tensor, train_y_tensor, len(int_index), 200, 50, 30, \"cuda:1\"\n",
    "perplexity_time_dual(test_X_tensor, test_lengths_tensor, test_y_tensor, len(int_index), 200, 50, 30, device, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The perplexity for the language generation model is slightly higher than the perplexity of the dual objective model: the first one gets 1.646 and the second one, 1.607. This means that the dual objective model performs somwehat better, which is suprising, given the results (accuracy and f1 score) for the character detection part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
