{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c01d2b0",
   "metadata": {},
   "source": [
    "                                                                                                Patricia Grau Francitorra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf5b08",
   "metadata": {},
   "source": [
    "# Assignment 1: Chinese character \"detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e6a11",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "Optical character recognition is an old \"AI\" and image-processing task.  What it involves is taking a photograph or scan of a piece of text (printed or handwritten) and turning the characters (as images) into character codes on the computer that therefore allow the text to be edited, indexed, etc.  A key part of that process is identifying where the characters actually are, especially if the characters are mixed among other non-writing, such as images of objects or people.\n",
    "\n",
    "In this assignment, you will **take images from a Chinese image database with annotations that indicate where the Chinese characters are, and you will train a model that takes test images, and superimposes upon them a visualization (of your choosing, e.g., a \"heat map\") of the likelihood that a pixel is close to or part of a valid Chinese character**.  The image database contains annotations of \"bounding boxes\", coordinates of the corners of a box that contains a single Chinese character.  In a sense, this assignment asks you to detect the bounding boxes in test images without the annotation, but a softer version of this: simply to **provide the probability, for each pixel, whether that pixel was part of a bounding box containing a Chinese character**.  Then, you are to \n",
    "   1. **superimpose upon the image a pixel-based map of likelihoods of where the bounding boxes ought to be** and\n",
    "   2. **apply an evaluation statistic**.\n",
    "\n",
    "This assignment grants you a lot of freedom in how you organize your code and set up the task overall.  Because of the degree of freedom it involves, it will mostly be graded on our evaluation of the effort put into the solution.  An actual high success at the task is not a requirement to get a high grade.  However, you will have to **report in detail**, in your own format, what you did, why you did it, how to run it -- it must run on mltgpu, be implemented in Python using PyTorch, and make use of the GPUs -- and how to apply it easily to our own test images.\n",
    "\n",
    "You will have almost a month to do this assignment, even though it is worth only 30% of your grade.  Another assignment with 30% will be given out for the last/remaining two weeks of the study period.   These time periods are coextensive with that of the project, but we expect you to be able to schedule your time well enough to put in an effort at both. This assignment is officially due at 23:59 on 2021 October 18. There are 30 points on this assignment, and a maximum of 20 bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa7228",
   "metadata": {},
   "source": [
    "**The data**\n",
    "\n",
    "The source of the task is here: https://ctwdataset.github.io/ (Links to an external site.) They have example images and an example of a baseline task that is much more advanced than what we are doing, but it will give you an idea of the data format, particularly the metadata.  Pay attention especially to the \"Annotation format\" section of this page: https://ctwdataset.github.io/tutorial/1-basics.html (Links to an external site.)\n",
    "\n",
    "The metadata and a small sample of the whole image dataset is available at /scratch/lt2326-h21/a1 on mltgpu. The metadata is in json format.  info.json contains information about every image file.  We will unzip only a minority of the original training image files.  train.jsonl is a list of json entities, one per line (that have to be parsed with the json package each separately) that correspond to the files in info.json.  This contains the bounding box information, as well as other information for the original challenge on the web.  See the \"Annotation format\" section mentioned on the dataset web page linked above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef73dd",
   "metadata": {},
   "source": [
    "## Part 1: data preparation (7 points)\n",
    "\n",
    "The image files are in /scratch/lt2326-h21/a1/images on mltgpu. They are in jpg format.  The code that you write for this part of the project should:\n",
    "\n",
    "- Use the info.json file to figure out what files are in the training set.  You will just use the official training data for everything.  Remember that you will only see a small minority of training examples in the images directory, for space reasons.\n",
    "- Divide up the official training data files into your own training, validation, and test datasets depending on your own preferences. You can choose to use fewer files than the maximum available if you run into problems with memory and so on (but first make sure your implementation is reasonably efficient).\n",
    "- Find the corresponding bounding box information in train.jsonl for each image. \n",
    "\n",
    "You can represent the data in any way you like, but remember that **it will become a numpy array for processing and a torch tensor for training**.  Remember also that **the classes are defined by pixel: for each pixel, you will eventually have a set of features (e.g. colour values), and a binary class corresponding to whether the pixel was in a Chinese character bounding box or not (note that there are non-Chinese characters in the set -- see the annotation instructions)**.  You are allowed to **reduce the dimensionality of the images for processing, but consider using a pooling and/or upsampling technique in Part 2** of this assignment to accomplish this goal. \n",
    "\n",
    "Describe the choices you made and the challenges you found in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d08cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.path as mplpath\n",
    "\n",
    "from skimage import transform\n",
    "\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c15a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some info\n",
    "\n",
    "# Opening JSON file\n",
    "f = open('/scratch/lt2326-h21/a1/info.json',)\n",
    "\n",
    "# returns JSON object as a dictionary\n",
    "info = json.load(f)\n",
    "\n",
    "# print(info.keys())\n",
    "# print(info['train'][0])\n",
    "# print(len(info['train']))\n",
    "\n",
    "# gets the names of the images in train\n",
    "im_in_train = []\n",
    "for a in info['train']:\n",
    "    name = a['image_id'] + '.jpg'\n",
    "    im_in_train.append(name)\n",
    "\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1503816",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/lt2326-h21/a1/train.jsonl') as trainfile:\n",
    "    traindata = [json.loads(x) for x in trainfile]\n",
    "\n",
    "# let's take a look at the traindata\n",
    "# traindata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1d2169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 845 images in the image folder that come from the train file.\n"
     ]
    }
   ],
   "source": [
    "# print(im_in_train)\n",
    "# checking what images are in the images folder\n",
    "\n",
    "path = '/scratch/lt2326-h21/a1/images'\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "usable_img_names = []\n",
    "for img_name in dirs:\n",
    "    if img_name in im_in_train:\n",
    "        usable_img_names.append(img_name)\n",
    "print('There are', len(usable_img_names), 'images in the image folder that come from the train file.')\n",
    "\n",
    "# only taking the images that are in the folder\n",
    "\n",
    "usable_data = []\n",
    "for e in traindata:\n",
    "    if e['file_name'] in usable_img_names:\n",
    "        usable_data.append(e)\n",
    "\n",
    "# print(usable_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7b0ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676 84 85\n"
     ]
    }
   ],
   "source": [
    "# separating data into training, testing and validation data\n",
    "# the train set consists of 80% of the data, test and validation sets are 10% of the data\n",
    "\n",
    "random.shuffle(usable_data)\n",
    "\n",
    "usable_train, usable_test, usable_val = np.split(usable_data, [int(len(usable_data)*0.8), int(len(usable_data)*0.9)])\n",
    "\n",
    "print(len(usable_train), len(usable_test), len(usable_val)) # sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7765697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have usable_train, a list that contains dictionaries for each image\n",
    "# {image_id1 : {'chinese' : [[poligon 1], [poligon2]], 'non_chinese' : [[poligon 1], [poligon2]]}, image_id2 : {'chinese' : [pol1], [pol2], [pol3]], 'non_chinese': []...}...}\n",
    "# We will have all the polygons of every image, the ones that contain chinese characters and the ones that don't\n",
    "\n",
    "def get_chinese(some_usable):\n",
    "    chinese_im = {}\n",
    "    \n",
    "    for image in some_usable:\n",
    "        chinese_im[image['image_id']] = {'chinese' : [], 'non_chinese' : []}\n",
    "        for anno in image['annotations']:\n",
    "            for a in anno:\n",
    "                if a['is_chinese']:\n",
    "                    chinese_im[image['image_id']]['chinese'].append(a['polygon'])\n",
    "                else:\n",
    "                    chinese_im[image['image_id']]['non_chinese'].append(a['polygon'])\n",
    "    return chinese_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e63e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_train = get_chinese(usable_train)\n",
    "chi_test = get_chinese(usable_test)\n",
    "chi_val = get_chinese(usable_val)\n",
    "\n",
    "train_im_names = list(chi_train.keys())\n",
    "test_im_names = list(chi_test.keys())\n",
    "val_im_names = list(chi_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa26d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for one image\n",
    "def imageAndTensor(image):  \n",
    "    path = '/scratch/lt2326-h21/a1/images'\n",
    "    \n",
    "    # loading the image\n",
    "    fullimg = mpimg.imread(path + \"/\" + image + \".jpg\")\n",
    "    img = transform.resize(fullimg, (200,200))\n",
    "\n",
    "    # making it a tensor\n",
    "    timg = torch.tensor(img)\n",
    "    timg = timg.float()\n",
    "\n",
    "    # preparing it for a conv layer\n",
    "    permute = timg.permute((2, 0, 1))\n",
    "    # unsqueezeme = permute.unsqueeze(0).to(device)\n",
    "\n",
    "    return permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05639f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just one image\n",
    "def ImageGoldValues(chi_data, image):\n",
    "   \n",
    "    # making an empty grid\n",
    "    grid = np.array([[[a,b] for b in list(range(2048))] for a in list(range(2048))])\n",
    "    grid.shape = (2048*2048, 2)\n",
    "\n",
    "    # getting the path for each polygon and updating a 0 matrix\n",
    "    zeros = np.zeros(2048*2048)\n",
    "\n",
    "    for pol in chi_data[image]['chinese']:\n",
    "        pol_path = mplpath.Path(pol)\n",
    "        truthvals = pol_path.contains_points(grid)\n",
    "\n",
    "        # turning it into 0s and 1s    \n",
    "        truthvals = np.asarray(truthvals, int)\n",
    "\n",
    "        # updating\n",
    "        # we take the maximum in case there is the same pixel in different polygons\n",
    "        zeros = np.maximum(truthvals, zeros)\n",
    "\n",
    "    return torch.LongTensor(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "545c8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d3c917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_parallel(chi_data, image):\n",
    "    return (imageAndTensor(image), \n",
    "            ImageGoldValues(chi_data, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab72533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data took 1477.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "processed_data = Parallel(n_jobs = 10)(delayed(process_images_in_parallel)(chi_train, image) for image in chi_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Processing data took\", round(end-start, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "883a12ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4194304])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "qwerty = imageAndTensor('0000566')\n",
    "uiop = ImageGoldValues(chi_train, '0000566')\n",
    "uiop.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a2893",
   "metadata": {},
   "source": [
    "## Part 2: the models (10 points)\n",
    "\n",
    "In this part, you will implement two substantially different model archictectures, that both take your representation of the images as training input and both take your representation of the bounding boxes as objective (HINT: the binary classification of pixels as belonging to a bounding box or not).  They will save the trained models to files so that they can be loaded and tested later. The output of the models will be a \"soft binary\" -- the probability of each pixel being inside a bounding box, from 0 to 1.  Consider examining some of the training data before designing your architectures.\n",
    "\n",
    "You have a large grant of freedom as to what these model architectures will look like (remember: grading is on a \"reasonable effort\" basis).  There's a high chance (HINT) that they will both use **one or more convolutional layers**, among other things.  Describe the models and the motivations for the architecture in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36c177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first model will be based in LeNet (1998), because baby steps y'know?\n",
    "# idea: two conv layers and three fully connected layers\n",
    "class FirstModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 3, (3, 3), 1, padding=(1, 1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(3, 3, (3, 3), 1, padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(3*50*50, 1*50*50)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.linear2 =  nn.Linear(50*50, 500)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.linear3 = nn.Linear(500, 100)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.upsampling = nn.Upsample((2048,2048))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu1(y)\n",
    "        y = self.pool1(y)\n",
    "        \n",
    "        y = self.conv2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool2(y)\n",
    "        \n",
    "#         y = y.reshape(3*50*50)\n",
    "        y = y.view(y.shape[0], -1)\n",
    "        \n",
    "        y = self.linear1(y)\n",
    "        y = self.relu3(y)\n",
    "        \n",
    "        y = self.linear2(y)\n",
    "        y = self.relu4(y)\n",
    "        \n",
    "        y = self.linear3(y)\n",
    "        y = self.relu5(y)\n",
    "        \n",
    "        reshaping = y.reshape((4, 10, 10, 1))\n",
    "        reshaping = reshaping.permute((0, 3, 1, 2)) # [4, 1, 10, 10]\n",
    "        \n",
    "        bigboi = self.upsampling(reshaping) # [4, 1, 2048, 2048]?\n",
    "        output = bigboi.permute((0, 2, 3, 1)) #[4, 2048, 2048, 1]?\n",
    "        output = output.reshape(4, 2048*2048)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "341e92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f319e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstmodel = FirstModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c304dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstmodel0 = FirstModel0().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3773235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "device = torch.device('cuda:3')\n",
    "\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67988859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(processed_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "626dbb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, device):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "#     loss = nn.LogSoftmax()\n",
    "    optimizer = optim.Adam(FirstModel().parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, (tensors, matrix) in enumerate(train_dataloader):\n",
    "            tensors = tensors.to(device)\n",
    "            matrix = matrix.long().to(device)\n",
    "\n",
    "            output = model(tensors)\n",
    "            print(output.size())\n",
    "            print(matrix.size())\n",
    "            \n",
    "            batch_loss = loss(output, matrix)\n",
    "            epoch_loss += batch_loss.item()\n",
    "            \n",
    "            print(epoch_loss/(i+1), end='\\r')\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85e6a5ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4194304])\n",
      "torch.Size([4, 4194304])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1534288/2039369126.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1534288/90387397.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1121\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "source": [
    "m1 = train(firstmodel, train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet\n",
    "\n",
    "class SecondModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SecondModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 3, (3, 3), 1, padding=(1, 1))\n",
    "        self.batchnorm1 = nn.BatchNorm2d(TODO)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(3, 3, (3, 3), 1, padding=(1, 1))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(TODO)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(3, 3, (3, 3), 1, padding=(1, 1))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(TODO)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(3, 3, (3, 3), 1, padding=(1, 1))\n",
    "        self.batchnorm4 = nn.BatchNorm2d(TODO)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(3, 3, (3, 3), 1, padding=(1, 1))\n",
    "        self.batchnorm5 = nn.BatchNorm2d(TODO)\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(TODO)\n",
    "        \n",
    "        self.linear1 = nn.Linear(TODO)\n",
    "        self.dropout1 = nn.Dropout(p=0.5) # 0.5?\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.linear2 =  nn.Linear(TODO)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.linear3 = nn.Linear(TODO)\n",
    "#         self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=TODO)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.conv1(x)\n",
    "        y = self.batchnorm1(y)\n",
    "        y = self.pool1(y)\n",
    "        \n",
    "        y = self.conv2(y)\n",
    "        y = self.batchnorm2(y)\n",
    "        y = self.pool2(y)\n",
    "        \n",
    "        y = self.conv3(y)\n",
    "        y = self.batchnorm3(y)\n",
    "        \n",
    "        y = self.conv4(y)\n",
    "        y = self.batchnorm4(y)\n",
    "        \n",
    "        y = self.conv5(y)\n",
    "        y = self.batchnorm5(y)\n",
    "        \n",
    "        y = self.pool3(y)\n",
    "        \n",
    "        y = y.reshape(TODO)\n",
    "        \n",
    "        y = self.linear1(y)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.relu1(y)\n",
    "        \n",
    "        y = self.linear2(y)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.relu2(y)\n",
    "        \n",
    "        y = self.linear3(y)\n",
    "        y = self.softmax(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f887f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6737549",
   "metadata": {},
   "source": [
    "don't forget File > Close and Halt to be good citizens ðŸ˜‡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
